import json
import os
import platform
import re
import subprocess
from dataclasses import dataclass
from enum import Enum
from typing import List, Optional

import yaml


class OS(Enum):
    WINDOWS = "Windows"
    MACOS = "Darwin"
    LINUX = "Linux"


class GPUVendor(Enum):
    NVIDIA = "NVIDIA"
    AMD = "AMD"
    INTEL = "Intel"
    APPLE = "Apple"
    UNKNOWN = "Unknown"


@dataclass
class CPUInfo:
    model: str
    cores: int
    threads: int
    frequency_ghz: float
    architecture: str
    features: List[str]


@dataclass
class GPUInfo:
    vendor: GPUVendor
    model: str
    vram_gb: float
    compute_capability: Optional[str] = None
    is_dedicated: bool = True


@dataclass
class MemoryInfo:
    total_gb: float
    available_gb: float
    type: str
    speed_mhz: Optional[float] = None


@dataclass
class HardwareInfo:
    os: OS
    cpu: CPUInfo
    gpu: Optional[GPUInfo]
    memory: MemoryInfo


@dataclass
class LlamaCppConfig:
    n_threads: int
    n_batch: int
    n_ctx: int
    n_gpu_layers: int
    quantization: str
    use_mmap: bool
    use_mlock: bool
    low_vram: bool
    split_mode: Optional[str] = None


@dataclass
class ConfigRecommendation:
    config: LlamaCppConfig
    reasoning: List[str]
    performance_expectation: str
    command_template: str


@dataclass
class ConfigValidation:
    is_valid: bool
    issues: List[str]
    warnings: List[str]
    suggestions: List[str]


class HardwareDetector:
    def __init__(self):
        self.os = self._detect_os()
        print(f"æ£€æµ‹åˆ°æ“ä½œç³»ç»Ÿ: {self.os.value}")
        print("-" * 80)

    def _detect_os(self) -> OS:
        system = platform.system()
        if system == "Windows":
            return OS.WINDOWS
        elif system == "Darwin":
            return OS.MACOS
        elif system == "Linux":
            return OS.LINUX
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: {system}")

    def detect_cpu(self) -> CPUInfo:
        print("æ­£åœ¨æ£€æµ‹CPUä¿¡æ¯...")
        try:
            if self.os == OS.WINDOWS:
                return self._detect_cpu_windows()
            elif self.os == OS.LINUX:
                return self._detect_cpu_linux()
            elif self.os == OS.MACOS:
                return self._detect_cpu_macos()
        except Exception as e:
            print(f"CPUæ£€æµ‹å¤±è´¥: {e}")
            return CPUInfo(
                model="Unknown",
                cores=4,
                threads=4,
                frequency_ghz=2.0,
                architecture="x86_64",
                features=[],
            )

    def _detect_cpu_windows(self) -> CPUInfo:
        import wmi

        c = wmi.WMI()
        cpu_info = c.Win32_Processor()[0]

        model = cpu_info.Name
        model = model.replace("(R)", "").replace("(TM)", "").strip()
        cores = cpu_info.NumberOfCores
        threads = cpu_info.NumberOfLogicalProcessors
        frequency_ghz = cpu_info.MaxClockSpeed / 1000.0

        features = []
        try:
            result = subprocess.run(
                ["wmic", "cpu", "get", "Capabilities", "/value"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                capabilities = result.stdout
                if "AVX" in capabilities:
                    features.append("AVX")
                if "AVX2" in capabilities:
                    features.append("AVX2")
                if "AVX512" in capabilities:
                    features.append("AVX512")
        except Exception:
            pass

        architecture = platform.machine()

        print(f"  CPUå‹å·: {model}")
        print(f"  ç‰©ç†æ ¸å¿ƒ: {cores}")
        print(f"  é€»è¾‘çº¿ç¨‹: {threads}")
        print(f"  ä¸»é¢‘: {frequency_ghz:.2f} GHz")
        print(f"  æ¶æ„: {architecture}")
        if features:
            print(f"  ç‰¹æ€§: {', '.join(features)}")

        return CPUInfo(
            model=model,
            cores=cores,
            threads=threads,
            frequency_ghz=frequency_ghz,
            architecture=architecture,
            features=features,
        )

    def _detect_cpu_linux(self) -> CPUInfo:
        model = "Unknown"
        cores = 0
        threads = 0
        frequency_ghz = 0.0
        features = []

        try:
            with open("/proc/cpuinfo", "r") as f:
                cpuinfo = f.read()

            model_match = re.search(r"model name\s*:\s*(.+)", cpuinfo)
            if model_match:
                model = model_match.group(1).strip()

            cores = len(re.findall(r"processor\s*:", cpuinfo))
            threads = cores

            freq_match = re.search(r"cpu MHz\s*:\s*([\d.]+)", cpuinfo)
            if freq_match:
                frequency_ghz = float(freq_match.group(1)) / 1000.0

            flags_match = re.search(r"flags\s*:\s*(.+)", cpuinfo)
            if flags_match:
                flags = flags_match.group(1)
                if "avx" in flags:
                    features.append("AVX")
                if "avx2" in flags:
                    features.append("AVX2")
                if "avx512f" in flags:
                    features.append("AVX512")

        except Exception as e:
            print(f"  è¯»å–/proc/cpuinfoå¤±è´¥: {e}")

        architecture = platform.machine()

        print(f"  CPUå‹å·: {model}")
        print(f"  é€»è¾‘çº¿ç¨‹: {threads}")
        print(f"  ä¸»é¢‘: {frequency_ghz:.2f} GHz")
        print(f"  æ¶æ„: {architecture}")
        if features:
            print(f"  ç‰¹æ€§: {', '.join(features)}")

        return CPUInfo(
            model=model,
            cores=cores,
            threads=threads,
            frequency_ghz=frequency_ghz,
            architecture=architecture,
            features=features,
        )

    def _detect_cpu_macos(self) -> CPUInfo:
        model = "Unknown"
        cores = 0
        threads = 0
        frequency_ghz = 0.0
        features = []

        try:
            result = subprocess.run(
                ["sysctl", "-n", "machdep.cpu.brand_string"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                model = result.stdout.strip()

            result = subprocess.run(
                ["sysctl", "-n", "hw.physicalcpu"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                cores = int(result.stdout.strip())

            result = subprocess.run(
                ["sysctl", "-n", "hw.logicalcpu"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                threads = int(result.stdout.strip())

            result = subprocess.run(
                ["sysctl", "-n", "hw.cpufrequency"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                frequency_ghz = float(result.stdout.strip()) / 1_000_000_000.0

            features.append("NEON")

        except Exception as e:
            print(f"  macOS CPUæ£€æµ‹å¤±è´¥: {e}")

        architecture = platform.machine()

        print(f"  CPUå‹å·: {model}")
        print(f"  ç‰©ç†æ ¸å¿ƒ: {cores}")
        print(f"  é€»è¾‘çº¿ç¨‹: {threads}")
        print(f"  ä¸»é¢‘: {frequency_ghz:.2f} GHz")
        print(f"  æ¶æ„: {architecture}")
        if features:
            print(f"  ç‰¹æ€§: {', '.join(features)}")

        return CPUInfo(
            model=model,
            cores=cores,
            threads=threads,
            frequency_ghz=frequency_ghz,
            architecture=architecture,
            features=features,
        )

    def detect_gpu(self) -> Optional[GPUInfo]:
        print("\næ­£åœ¨æ£€æµ‹GPUä¿¡æ¯...")
        try:
            if self.os == OS.WINDOWS:
                return self._detect_gpu_windows()
            elif self.os == OS.LINUX:
                return self._detect_gpu_linux()
            elif self.os == OS.MACOS:
                return self._detect_gpu_macos()
        except Exception as e:
            print(f"GPUæ£€æµ‹å¤±è´¥: {e}")
            return None

    def _detect_gpu_windows(self) -> Optional[GPUInfo]:
        import wmi

        c = wmi.WMI()

        gpus = []
        for gpu in c.Win32_VideoController():
            if gpu.Name and "Microsoft Basic Display Adapter" not in gpu.Name:
                vendor = self._detect_gpu_vendor(gpu.Name)
                vram_gb = gpu.AdapterRAM / (1024**3) if gpu.AdapterRAM else 0.0
                gpus.append(
                    GPUInfo(
                        vendor=vendor,
                        model=gpu.Name,
                        vram_gb=vram_gb,
                        is_dedicated=vram_gb > 1.0,
                    )
                )

        if gpus:
            gpu = gpus[0]
            print(f"  GPUå‹å·: {gpu.model}")
            print(f"  å‚å•†: {gpu.vendor.value}")
            print(f"  æ˜¾å­˜: {gpu.vram_gb:.1f} GB")
            print(f"  ç±»å‹: {'ç‹¬ç«‹æ˜¾å¡' if gpu.is_dedicated else 'é›†æˆæ˜¾å¡'}")
            return gpu

        print("  æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPU")
        return None

    def _detect_gpu_linux(self) -> Optional[GPUInfo]:
        try:
            result = subprocess.run(
                ["lspci", "-nn"], capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                output = result.stdout

                nvidia_match = re.search(
                    r"VGA.*NVIDIA.*\[(.+?)\].*?(\d+)MiB", output, re.IGNORECASE
                )
                if nvidia_match:
                    model = nvidia_match.group(1)
                    vram_gb = int(nvidia_match.group(2)) / 1024.0
                    print(f"  GPUå‹å·: {model}")
                    print("  å‚å•†: NVIDIA")
                    print(f"  æ˜¾å­˜: {vram_gb:.1f} GB")
                    print("  ç±»å‹: ç‹¬ç«‹æ˜¾å¡")
                    return GPUInfo(
                        vendor=GPUVendor.NVIDIA, model=model, vram_gb=vram_gb
                    )

                amd_match = re.search(
                    r"VGA.*AMD.*\[(.+?)\].*?(\d+)MiB", output, re.IGNORECASE
                )
                if amd_match:
                    model = amd_match.group(1)
                    vram_gb = int(amd_match.group(2)) / 1024.0
                    print(f"  GPUå‹å·: {model}")
                    print("  å‚å•†: AMD")
                    print(f"  æ˜¾å­˜: {vram_gb:.1f} GB")
                    print("  ç±»å‹: ç‹¬ç«‹æ˜¾å¡")
                    return GPUInfo(vendor=GPUVendor.AMD, model=model, vram_gb=vram_gb)

        except Exception as e:
            print(f"  Linux GPUæ£€æµ‹å¤±è´¥: {e}")

        print("  æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPU")
        return None

    def _detect_gpu_macos(self) -> Optional[GPUInfo]:
        try:
            result = subprocess.run(
                ["system_profiler", "SPDisplaysDataType", "-json"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)
                if data.get("SPDisplaysDataType"):
                    gpu_data = data["SPDisplaysDataType"][0]
                    model = gpu_data.get("sppci_model", "Unknown")
                    vram_mb = gpu_data.get("sppci_vram_mb", 0)
                    vram_gb = vram_mb / 1024.0

                    print(f"  GPUå‹å·: {model}")
                    print("  å‚å•†: Apple")
                    print(f"  æ˜¾å­˜: {vram_gb:.1f} GB")
                    print(f"  ç±»å‹: {'ç‹¬ç«‹æ˜¾å¡' if vram_gb > 1.0 else 'é›†æˆæ˜¾å¡'}")

                    return GPUInfo(
                        vendor=GPUVendor.APPLE,
                        model=model,
                        vram_gb=vram_gb,
                        is_dedicated=vram_gb > 1.0,
                    )

        except Exception as e:
            print(f"  macOS GPUæ£€æµ‹å¤±è´¥: {e}")

        print("  æœªæ£€æµ‹åˆ°ç‹¬ç«‹GPU")
        return None

    def _detect_gpu_vendor(self, gpu_name: str) -> GPUVendor:
        gpu_name_lower = gpu_name.lower()
        if (
            "nvidia" in gpu_name_lower
            or "geforce" in gpu_name_lower
            or "rtx" in gpu_name_lower
        ):
            return GPUVendor.NVIDIA
        elif "amd" in gpu_name_lower or "radeon" in gpu_name_lower:
            return GPUVendor.AMD
        elif "intel" in gpu_name_lower:
            return GPUVendor.INTEL
        else:
            return GPUVendor.UNKNOWN

    def detect_memory(self) -> MemoryInfo:
        print("\næ­£åœ¨æ£€æµ‹å†…å­˜ä¿¡æ¯...")
        try:
            if self.os == OS.WINDOWS:
                return self._detect_memory_windows()
            elif self.os == OS.LINUX:
                return self._detect_memory_linux()
            elif self.os == OS.MACOS:
                return self._detect_memory_macos()
        except Exception as e:
            print(f"å†…å­˜æ£€æµ‹å¤±è´¥: {e}")
            return MemoryInfo(total_gb=8.0, available_gb=4.0, type="DDR4")

    def _detect_memory_windows(self) -> MemoryInfo:
        import psutil

        mem = psutil.virtual_memory()
        total_gb = mem.total / (1024**3)
        available_gb = mem.available / (1024**3)

        memory_type = "DDR4"
        speed_mhz = None

        try:
            import wmi

            c = wmi.WMI()
            for mem_module in c.Win32_PhysicalMemory():
                if mem_module.Speed:
                    speed_mhz = mem_module.Speed
                if mem_module.MemoryType:
                    memory_types = {
                        0: "Unknown",
                        1: "Other",
                        2: "DRAM",
                        3: "Synchronous DRAM",
                        4: "Cache DRAM",
                        5: "EDO",
                        6: "EDRAM",
                        7: "VRAM",
                        8: "SRAM",
                        9: "RAM",
                        10: "ROM",
                        11: "Flash",
                        12: "EEPROM",
                        13: "FEPROM",
                        14: "EPROM",
                        15: "CDRAM",
                        16: "3DRAM",
                        17: "SDRAM",
                        18: "SGRAM",
                        19: "RDRAM",
                        20: "DDR",
                        21: "DDR2",
                        22: "DDR2 FB-DIMM",
                        24: "DDR3",
                        26: "DDR4",
                        27: "DDR5",
                    }
                    memory_type = memory_types.get(mem_module.MemoryType, "Unknown")
                break

        except Exception as e:
            print(f"  è·å–å†…å­˜ç±»å‹å¤±è´¥: {e}")

        print(f"  æ€»å†…å­˜: {total_gb:.1f} GB")
        print(f"  å¯ç”¨å†…å­˜: {available_gb:.1f} GB")
        print(f"  å†…å­˜ç±»å‹: {memory_type}")
        if speed_mhz:
            print(f"  å†…å­˜é¢‘ç‡: {speed_mhz:.0f} MHz")

        return MemoryInfo(
            total_gb=total_gb,
            available_gb=available_gb,
            type=memory_type,
            speed_mhz=speed_mhz,
        )

    def _detect_memory_linux(self) -> MemoryInfo:
        import psutil

        mem = psutil.virtual_memory()
        total_gb = mem.total / (1024**3)
        available_gb = mem.available / (1024**3)

        memory_type = "DDR4"
        speed_mhz = None

        try:
            result = subprocess.run(
                ["sudo", "dmidecode", "-t", "memory"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                output = result.stdout
                type_match = re.search(r"Type: (.+)", output)
                if type_match:
                    memory_type = type_match.group(1).strip()
                speed_match = re.search(r"Speed: (\d+) MHz", output)
                if speed_match:
                    speed_mhz = float(speed_match.group(1))
        except Exception as e:
            print(f"  è·å–å†…å­˜ç±»å‹å¤±è´¥: {e}")

        print(f"  æ€»å†…å­˜: {total_gb:.1f} GB")
        print(f"  å¯ç”¨å†…å­˜: {available_gb:.1f} GB")
        print(f"  å†…å­˜ç±»å‹: {memory_type}")
        if speed_mhz:
            print(f"  å†…å­˜é¢‘ç‡: {speed_mhz:.0f} MHz")

        return MemoryInfo(
            total_gb=total_gb,
            available_gb=available_gb,
            type=memory_type,
            speed_mhz=speed_mhz,
        )

    def _detect_memory_macos(self) -> MemoryInfo:
        import psutil

        mem = psutil.virtual_memory()
        total_gb = mem.total / (1024**3)
        available_gb = mem.available / (1024**3)

        memory_type = "DDR4"
        speed_mhz = None

        try:
            result = subprocess.run(
                ["system_profiler", "SPMemoryDataType", "-json"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)
                if data.get("SPMemoryDataType"):
                    mem_data = data["SPMemoryDataType"][0]
                    memory_type = mem_data.get("dimm_type", "Unknown")
        except Exception as e:
            print(f"  è·å–å†…å­˜ç±»å‹å¤±è´¥: {e}")

        print(f"  æ€»å†…å­˜: {total_gb:.1f} GB")
        print(f"  å¯ç”¨å†…å­˜: {available_gb:.1f} GB")
        print(f"  å†…å­˜ç±»å‹: {memory_type}")

        return MemoryInfo(
            total_gb=total_gb,
            available_gb=available_gb,
            type=memory_type,
            speed_mhz=speed_mhz,
        )

    def detect_all(self) -> HardwareInfo:
        print("=" * 80)
        print("å¼€å§‹ç¡¬ä»¶æ£€æµ‹")
        print("=" * 80)

        cpu = self.detect_cpu()
        gpu = self.detect_gpu()
        memory = self.detect_memory()

        print("\n" + "=" * 80)
        print("ç¡¬ä»¶æ£€æµ‹å®Œæˆ")
        print("=" * 80)

        return HardwareInfo(os=self.os, cpu=cpu, gpu=gpu, memory=memory)


class LlamaCppConfigOptimizer:
    def __init__(self, hardware: HardwareInfo, config_path: Optional[str] = None):
        self.hardware = hardware
        self.config_path = config_path
        self.current_config = None
        if config_path:
            self._load_current_config()

    def _load_current_config(self) -> None:
        """åŠ è½½å½“å‰é…ç½®æ–‡ä»¶"""
        if not self.config_path or not os.path.exists(self.config_path):
            return

        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                config_data = yaml.safe_load(f)

            if config_data and "local_llm" in config_data:
                llm_config = config_data["local_llm"]
                self.current_config = {
                    "n_threads": llm_config.get("n_threads"),
                    "n_batch": llm_config.get("n_batch"),
                    "n_ctx": llm_config.get("n_ctx"),
                    "n_gpu_layers": llm_config.get("n_gpu_layers"),
                    "use_mmap": llm_config.get("use_mmap", True),
                    "use_mlock": llm_config.get("use_mlock", False),
                }
                print(f"å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
        except Exception as e:
            print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")

    def validate_current_config(
        self, recommended_config: LlamaCppConfig
    ) -> ConfigValidation:
        """éªŒè¯å½“å‰é…ç½®å¹¶æä¾›å»ºè®®"""
        issues = []
        warnings = []
        suggestions = []

        if not self.current_config:
            return ConfigValidation(
                is_valid=True, issues=[], warnings=[], suggestions=[]
            )

        cpu = self.hardware.cpu
        gpu = self.hardware.gpu

        current_n_threads = self.current_config.get("n_threads")
        if current_n_threads:
            if current_n_threads > cpu.threads:
                issues.append(
                    f"çº¿ç¨‹æ•° ({current_n_threads}) è¶…è¿‡CPUé€»è¾‘çº¿ç¨‹æ•° "
                    f"({cpu.threads})ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™"
                )
            elif current_n_threads == cpu.threads:
                warnings.append(
                    f"çº¿ç¨‹æ•° ({current_n_threads}) ç­‰äºCPUé€»è¾‘çº¿ç¨‹æ•°ï¼Œ"
                    f"å»ºè®®é¢„ç•™1ä¸ªçº¿ç¨‹ç»™ç³»ç»Ÿ"
                )

        current_n_batch = self.current_config.get("n_batch")
        if current_n_batch:
            if gpu and gpu.vram_gb >= 8.0 and current_n_batch < 256:
                suggestions.append(
                    f"GPUæ˜¾å­˜å……è¶³ ({gpu.vram_gb:.1f}GB)ï¼Œ"
                    f"å»ºè®®å¢åŠ æ‰¹å¤„ç†å¤§å°åˆ°256æˆ–512ä»¥æå‡ååé‡"
                )
            elif not gpu and current_n_batch > 256:
                warnings.append(
                    f"æ— ç‹¬ç«‹GPUï¼Œæ‰¹å¤„ç†å¤§å° ({current_n_batch}) è¾ƒå¤§ï¼Œå¯èƒ½å ç”¨è¿‡å¤šå†…å­˜"
                )

        current_n_ctx = self.current_config.get("n_ctx")
        if current_n_ctx:
            if current_n_ctx > 8192:
                warnings.append(f"ä¸Šä¸‹æ–‡çª—å£ ({current_n_ctx}) è¾ƒå¤§ï¼Œå°†å ç”¨æ›´å¤šå†…å­˜")
            if current_n_ctx < 2048:
                suggestions.append(
                    f"ä¸Šä¸‹æ–‡çª—å£ ({current_n_ctx}) è¾ƒå°ï¼Œ"
                    f"å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆå»ºè®®å¢åŠ åˆ°4096æˆ–8192"
                )

        current_n_gpu_layers = self.current_config.get("n_gpu_layers")
        if current_n_gpu_layers is not None:
            if gpu and gpu.vram_gb >= 8.0 and current_n_gpu_layers == 0:
                issues.append(
                    f"GPUæ˜¾å­˜å……è¶³ ({gpu.vram_gb:.1f}GB)ï¼Œ"
                    f"ä½†GPUå±‚æ•°è®¾ç½®ä¸º0ï¼Œå»ºè®®å¯ç”¨GPUåŠ é€Ÿ"
                )
            elif gpu and gpu.vram_gb < 4.0 and current_n_gpu_layers > 30:
                warnings.append(
                    f"GPUæ˜¾å­˜è¾ƒå° ({gpu.vram_gb:.1f}GB)ï¼Œ"
                    f"GPUå±‚æ•° ({current_n_gpu_layers}) å¯èƒ½å¯¼è‡´æ˜¾å­˜ä¸è¶³"
                )

        is_valid = len(issues) == 0

        return ConfigValidation(
            is_valid=is_valid, issues=issues, warnings=warnings, suggestions=suggestions
        )

    def compare_configs(self, recommended_config: LlamaCppConfig) -> None:
        """å¯¹æ¯”å½“å‰é…ç½®å’Œæ¨èé…ç½®"""
        if not self.current_config:
            print("\næœªæ‰¾åˆ°å½“å‰é…ç½®æ–‡ä»¶ï¼Œè·³è¿‡å¯¹æ¯”")
            return

        print("\n" + "=" * 80)
        print("é…ç½®å¯¹æ¯”")
        print("=" * 80)

        print(f"\n{'å‚æ•°':<25} {'å½“å‰é…ç½®':<15} {'æ¨èé…ç½®':<15} {'çŠ¶æ€'}")
        print("-" * 80)

        params = [
            ("çº¿ç¨‹æ•° (-t)", "n_threads", recommended_config.n_threads),
            ("æ‰¹å¤„ç†å¤§å° (-b)", "n_batch", recommended_config.n_batch),
            ("ä¸Šä¸‹æ–‡çª—å£ (-c)", "n_ctx", recommended_config.n_ctx),
            ("GPUå±‚æ•° (-ngl)", "n_gpu_layers", recommended_config.n_gpu_layers),
            ("å†…å­˜æ˜ å°„", "use_mmap", recommended_config.use_mmap),
            ("å†…å­˜é”å®š", "use_mlock", recommended_config.use_mlock),
        ]

        for param_name, config_key, recommended_value in params:
            current_value = self.current_config.get(config_key)
            if current_value is None:
                status = "æœªè®¾ç½®"
            elif current_value == recommended_value:
                status = "âœ… åŒ¹é…"
            else:
                status = "âš ï¸ å·®å¼‚"

            current_str = str(current_value) if current_value is not None else "æœªè®¾ç½®"
            recommended_str = str(recommended_value)

            print(f"{param_name:<25} {current_str:<15} {recommended_str:<15} {status}")

        validation = self.validate_current_config(recommended_config)

        if validation.issues:
            print("\n" + "=" * 80)
            print("é…ç½®é—®é¢˜")
            print("=" * 80)
            for i, issue in enumerate(validation.issues, 1):
                print(f"  âŒ {i}. {issue}")

        if validation.warnings:
            print("\n" + "=" * 80)
            print("é…ç½®è­¦å‘Š")
            print("=" * 80)
            for i, warning in enumerate(validation.warnings, 1):
                print(f"  âš ï¸ {i}. {warning}")

        if validation.suggestions:
            print("\n" + "=" * 80)
            print("ä¼˜åŒ–å»ºè®®")
            print("=" * 80)
            for i, suggestion in enumerate(validation.suggestions, 1):
                print(f"  ğŸ’¡ {i}. {suggestion}")

    def recommend_config(self) -> ConfigRecommendation:
        print("\n" + "=" * 80)
        print("ç”Ÿæˆllama-cppé…ç½®æ¨è")
        print("=" * 80)

        reasoning = []
        config = self._calculate_optimal_config(reasoning)
        performance = self._estimate_performance(config)
        command = self._generate_command(config)

        print("\næ¨èé…ç½®:")
        print("-" * 80)
        print(f"  çº¿ç¨‹æ•°æ•° (-t): {config.n_threads}")
        print(f"  æ‰¹å¤„ç†å¤§å° (-b): {config.n_batch}")
        print(f"  ä¸Šä¸‹æ–‡çª—å£ (-c): {config.n_ctx}")
        print(f"  GPUå±‚æ•° (-ngl): {config.n_gpu_layers}")
        print(f"  é‡åŒ–çº§åˆ«: {config.quantization}")
        print(f"  å†…å­˜æ˜ å°„: {'å¯ç”¨' if config.use_mmap else 'ç¦ç”¨'}")
        print(f"  å†…å­˜é”å®š: {'å¯ç”¨' if config.use_mlock else 'ç¦ç”¨'}")
        print(f"  ä½æ˜¾å­˜æ¨¡å¼: {'å¯ç”¨' if config.low_vram else 'ç¦ç”¨'}")

        print("\næ¨èä¾æ®:")
        print("-" * 80)
        for i, reason in enumerate(reasoning, 1):
            print(f"  {i}. {reason}")

        print("\næ€§èƒ½é¢„æœŸ:")
        print("-" * 80)
        print(f"  {performance}")

        print("\nå‘½ä»¤è¡Œç¤ºä¾‹:")
        print("-" * 80)
        print(command)

        return ConfigRecommendation(
            config=config,
            reasoning=reasoning,
            performance_expectation=performance,
            command_template=command,
        )

    def _calculate_optimal_config(self, reasoning: List[str]) -> LlamaCppConfig:
        cpu = self.hardware.cpu
        gpu = self.hardware.gpu
        memory = self.hardware.memory

        n_threads = self._calculate_threads(cpu, reasoning)
        n_batch = self._calculate_batch_size(cpu, gpu, reasoning)
        n_ctx = self._calculate_context_size(memory, gpu, reasoning)
        n_gpu_layers = self._calculate_gpu_layers(gpu, reasoning)
        quantization = self._calculate_quantization(memory, gpu, reasoning)
        use_mmap = self._should_use_mmap(memory, reasoning)
        use_mlock = self._should_use_mlock(self.hardware.os, reasoning)
        low_vram = self._should_use_low_vram(gpu, memory, reasoning)
        split_mode = self._calculate_split_mode(gpu, reasoning)

        return LlamaCppConfig(
            n_threads=n_threads,
            n_batch=n_batch,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            quantization=quantization,
            use_mmap=use_mmap,
            use_mlock=use_mlock,
            low_vram=low_vram,
            split_mode=split_mode,
        )

    def _calculate_threads(self, cpu: CPUInfo, reasoning: List[str]) -> int:
        n_threads = max(1, cpu.threads - 1)
        reasoning.append(
            f"çº¿ç¨‹æ•°è®¾ç½®ä¸º{n_threads}ï¼ˆCPUé€»è¾‘çº¿ç¨‹æ•°{cpu.threads}ï¼Œé¢„ç•™1ä¸ªçº¿ç¨‹ç»™ç³»ç»Ÿï¼‰"
        )
        return n_threads

    def _calculate_batch_size(
        self, cpu: CPUInfo, gpu: Optional[GPUInfo], reasoning: List[str]
    ) -> int:
        if gpu and gpu.vram_gb >= 8.0:
            n_batch = 512
            reasoning.append(
                f"æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º{n_batch}ï¼ˆGPUæ˜¾å­˜å……è¶³{gpu.vram_gb:.1f}GBï¼Œä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡æå‡ååé‡ï¼‰"
            )
        elif gpu and gpu.vram_gb >= 4.0:
            n_batch = 256
            reasoning.append(
                f"æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º{n_batch}ï¼ˆGPUæ˜¾å­˜é€‚ä¸­{gpu.vram_gb:.1f}GBï¼Œå¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜ä½¿ç”¨ï¼‰"
            )
        else:
            n_batch = 128
            reasoning.append(
                f"æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º{n_batch}ï¼ˆæ— ç‹¬ç«‹GPUæˆ–æ˜¾å­˜è¾ƒå°ï¼Œä½¿ç”¨è¾ƒå°æ‰¹æ¬¡å‡å°‘å†…å­˜å ç”¨ï¼‰"
            )

        return n_batch

    def _calculate_context_size(
        self, memory: MemoryInfo, gpu: Optional[GPUInfo], reasoning: List[str]
    ) -> int:
        if gpu and gpu.vram_gb >= 16.0:
            n_ctx = 8192
            reasoning.append(
                f"ä¸Šä¸‹æ–‡çª—å£è®¾ç½®ä¸º{n_ctx}ï¼ˆGPUæ˜¾å­˜å……è¶³{gpu.vram_gb:.1f}GBï¼Œæ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼‰"
            )
        elif gpu and gpu.vram_gb >= 8.0:
            n_ctx = 4096
            reasoning.append(
                f"ä¸Šä¸‹æ–‡çª—å£è®¾ç½®ä¸º{n_ctx}ï¼ˆGPUæ˜¾å­˜é€‚ä¸­{gpu.vram_gb:.1f}GBï¼Œæ ‡å‡†ä¸Šä¸‹æ–‡é•¿åº¦ï¼‰"
            )
        elif memory.total_gb >= 32.0:
            n_ctx = 4096
            reasoning.append(
                f"ä¸Šä¸‹æ–‡çª—å£è®¾ç½®ä¸º{n_ctx}ï¼ˆå†…å­˜å……è¶³{memory.total_gb:.1f}GBï¼Œæ ‡å‡†ä¸Šä¸‹æ–‡é•¿åº¦ï¼‰"
            )
        else:
            n_ctx = 2048
            reasoning.append(
                f"ä¸Šä¸‹æ–‡çª—å£è®¾ç½®ä¸º{n_ctx}ï¼ˆå†…å­˜æœ‰é™{memory.total_gb:.1f}GBï¼Œä½¿ç”¨è¾ƒçŸ­ä¸Šä¸‹æ–‡ï¼‰"
            )

        return n_ctx

    def _calculate_gpu_layers(
        self, gpu: Optional[GPUInfo], reasoning: List[str]
    ) -> int:
        if not gpu:
            n_gpu_layers = 0
            reasoning.append("GPUå±‚æ•°è®¾ç½®ä¸º0ï¼ˆæœªæ£€æµ‹åˆ°ç‹¬ç«‹GPUï¼Œä½¿ç”¨çº¯CPUæ¨ç†ï¼‰")
        elif gpu.vram_gb >= 16.0:
            n_gpu_layers = 99
            reasoning.append(
                f"GPUå±‚æ•°è®¾ç½®ä¸º{n_gpu_layers}ï¼ˆGPUæ˜¾å­˜å……è¶³{gpu.vram_gb:.1f}GBï¼Œå°†æ‰€æœ‰å±‚åŠ è½½åˆ°GPUï¼‰"
            )
        elif gpu.vram_gb >= 8.0:
            n_gpu_layers = 50
            reasoning.append(
                f"GPUå±‚æ•°è®¾ç½®ä¸º{n_gpu_layers}ï¼ˆGPUæ˜¾å­˜é€‚ä¸­{gpu.vram_gb:.1f}GBï¼Œå°†éƒ¨åˆ†å±‚åŠ è½½åˆ°GPUï¼‰"
            )
        elif gpu.vram_gb >= 4.0:
            n_gpu_layers = 30
            reasoning.append(
                f"GPUå±‚æ•°è®¾ç½®ä¸º{n_gpu_layers}ï¼ˆGPUæ˜¾å­˜è¾ƒå°{gpu.vram_gb:.1f}GBï¼Œä»…å°†éƒ¨åˆ†å±‚åŠ è½½åˆ°GPUï¼‰"
            )
        else:
            n_gpu_layers = 0
            reasoning.append(
                f"GPUå±‚æ•°è®¾ç½®ä¸º0ï¼ˆGPUæ˜¾å­˜ä¸è¶³{gpu.vram_gb:.1f}GBï¼Œä½¿ç”¨çº¯CPUæ¨ç†ï¼‰"
            )

        return n_gpu_layers

    def _calculate_quantization(
        self, memory: MemoryInfo, gpu: Optional[GPUInfo], reasoning: List[str]
    ) -> str:
        if gpu and gpu.vram_gb >= 16.0:
            quantization = "Q4_K_M"
            reasoning.append(
                f"é‡åŒ–çº§åˆ«è®¾ç½®ä¸º{quantization}ï¼ˆGPUæ˜¾å­˜å……è¶³{gpu.vram_gb:.1f}GBï¼Œä½¿ç”¨Q4é‡åŒ–å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ï¼‰"
            )
        elif gpu and gpu.vram_gb >= 8.0:
            quantization = "Q4_K_M"
            reasoning.append(
                f"é‡åŒ–çº§åˆ«è®¾ç½®ä¸º{quantization}ï¼ˆGPUæ˜¾å­˜é€‚ä¸­{gpu.vram_gb:.1f}GBï¼Œä½¿ç”¨Q4é‡åŒ–ï¼‰"
            )
        elif memory.total_gb >= 32.0:
            quantization = "Q4_K_M"
            reasoning.append(
                f"é‡åŒ–çº§åˆ«è®¾ç½®ä¸º{quantization}ï¼ˆå†…å­˜å……è¶³{memory.total_gb:.1f}GBï¼Œä½¿ç”¨Q4é‡åŒ–ï¼‰"
            )
        elif memory.total_gb >= 16.0:
            quantization = "Q5_K_M"
            reasoning.append(
                f"é‡åŒ–çº§åˆ«è®¾ç½®ä¸º{quantization}ï¼ˆå†…å­˜é€‚ä¸­{memory.total_gb:.1f}GBï¼Œä½¿ç”¨Q5é‡åŒ–æå‡è´¨é‡ï¼‰"
            )
        else:
            quantization = "Q6_K"
            reasoning.append(
                f"é‡åŒ–çº§åˆ«è®¾ç½®ä¸º{quantization}ï¼ˆå†…å­˜æœ‰é™{memory.total_gb:.1f}GBï¼Œä½¿ç”¨Q6é‡åŒ–å‡å°‘å†…å­˜å ç”¨ï¼‰"
            )

        return quantization

    def _should_use_mmap(self, memory: MemoryInfo, reasoning: List[str]) -> bool:
        use_mmap = memory.total_gb >= 8.0
        if use_mmap:
            reasoning.append(
                f"å¯ç”¨å†…å­˜æ˜ å°„ï¼ˆå†…å­˜{memory.total_gb:.1f}GBå……è¶³ï¼Œmmapå¯å‡å°‘å†…å­˜å ç”¨ï¼‰"
            )
        else:
            reasoning.append(
                f"ç¦ç”¨å†…å­˜æ˜ å°„ï¼ˆå†…å­˜{memory.total_gb:.1f}GBæœ‰é™ï¼Œé¿å…é¢‘ç¹I/Oï¼‰"
            )

        return use_mmap

    def _should_use_mlock(self, os: OS, reasoning: List[str]) -> bool:
        use_mlock = os != OS.WINDOWS
        if use_mlock:
            reasoning.append("å¯ç”¨å†…å­˜é”å®šï¼ˆéWindowsç³»ç»Ÿï¼Œmlockå¯é˜²æ­¢å†…å­˜è¢«äº¤æ¢ï¼‰")
        else:
            reasoning.append("ç¦ç”¨å†…å­˜é”å®šï¼ˆWindowsç³»ç»Ÿä¸æ”¯æŒmlockï¼‰")

        return use_mlock

    def _should_use_low_vram(
        self, gpu: Optional[GPUInfo], memory: MemoryInfo, reasoning: List[str]
    ) -> bool:
        if not gpu:
            low_vram = False
            reasoning.append("ç¦ç”¨ä½æ˜¾å­˜æ¨¡å¼ï¼ˆæœªæ£€æµ‹åˆ°GPUï¼‰")
        elif gpu.vram_gb >= 8.0:
            low_vram = False
            reasoning.append(f"ç¦ç”¨ä½æ˜¾å­˜æ¨¡å¼ï¼ˆGPUæ˜¾å­˜å……è¶³{gpu.vram_gb:.1f}GBï¼‰")
        else:
            low_vram = True
            reasoning.append(
                f"å¯ç”¨ä½æ˜¾å­˜æ¨¡å¼ï¼ˆGPUæ˜¾å­˜è¾ƒå°{gpu.vram_gb:.1f}GBï¼Œå‡å°‘æ˜¾å­˜å ç”¨ï¼‰"
            )

        return low_vram

    def _calculate_split_mode(
        self, gpu: Optional[GPUInfo], reasoning: List[str]
    ) -> Optional[str]:
        if not gpu:
            return None

        if gpu.vendor == GPUVendor.NVIDIA and gpu.vram_gb >= 8.0:
            split_mode = "layer"
            reasoning.append(
                f"ä½¿ç”¨åˆ†å±‚åˆ†å‰²æ¨¡å¼ï¼ˆNVIDIA GPUæ˜¾å­˜å……è¶³{gpu.vram_gb:.1f}GBï¼Œ"
                f"åˆ†å±‚åˆ†å‰²ä¼˜åŒ–æ€§èƒ½ï¼‰"
            )
            return split_mode

        return None

    def _estimate_performance(self, config: LlamaCppConfig) -> str:
        gpu = self.hardware.gpu
        cpu = self.hardware.cpu

        if gpu and config.n_gpu_layers > 0:
            if gpu.vram_gb >= 16.0:
                return "é«˜æ€§èƒ½ï¼šGPUåŠ é€Ÿæ¨ç†ï¼Œé¢„æœŸç”Ÿæˆé€Ÿåº¦30-50 tokens/ç§’ï¼Œé€‚åˆå®æ—¶å¯¹è¯"
            elif gpu.vram_gb >= 8.0:
                return (
                    "ä¸­é«˜æ€§èƒ½ï¼šGPU+CPUæ··åˆæ¨ç†ï¼Œ"
                    "é¢„æœŸç”Ÿæˆé€Ÿåº¦20-30 tokens/ç§’ï¼Œé€‚åˆäº¤äº’å¼åº”ç”¨"
                )
            else:
                return (
                    "ä¸­ç­‰æ€§èƒ½ï¼šGPUè¾…åŠ©æ¨ç†ï¼Œé¢„æœŸç”Ÿæˆé€Ÿåº¦10-20 tokens/ç§’ï¼Œé€‚åˆæ‰¹å¤„ç†ä»»åŠ¡"
                )
        else:
            if cpu.threads >= 8:
                return "ä¸­ç­‰æ€§èƒ½ï¼šçº¯CPUæ¨ç†ï¼Œé¢„æœŸç”Ÿæˆé€Ÿåº¦5-10 tokens/ç§’ï¼Œé€‚åˆç¦»çº¿å¤„ç†"
            else:
                return "åŸºç¡€æ€§èƒ½ï¼šçº¯CPUæ¨ç†ï¼Œé¢„æœŸç”Ÿæˆé€Ÿåº¦2-5 tokens/ç§’ï¼Œé€‚åˆå°è§„æ¨¡æµ‹è¯•"

    def _generate_command(self, config: LlamaCppConfig) -> str:
        command_parts = ["llama-cli"]

        command_parts.append("-m model.gguf")
        command_parts.append(f"-t {config.n_threads}")
        command_parts.append(f"-b {config.n_batch}")
        command_parts.append(f"-c {config.n_ctx}")
        command_parts.append(f"-ngl {config.n_gpu_layers}")

        if config.use_mmap:
            command_parts.append("--mmap")
        if config.use_mlock:
            command_parts.append("--mlock")
        if config.low_vram:
            command_parts.append("--low-vram")
        if config.split_mode:
            command_parts.append(f"--split-mode {config.split_mode}")

        command = " ".join(command_parts)

        return command


def main():
    import argparse

    parser = argparse.ArgumentParser(description="llama-cpp é…ç½®ä¼˜åŒ–å™¨")
    parser.add_argument(
        "--config",
        "-c",
        type=str,
        default="config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šconfig.yamlï¼‰",
    )
    args = parser.parse_args()

    print("=" * 80)
    print("llama-cpp é…ç½®ä¼˜åŒ–å™¨")
    print("=" * 80)
    print()

    try:
        detector = HardwareDetector()
        hardware = detector.detect_all()

        config_path = args.config
        if os.path.exists(config_path):
            print(f"\nä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
        else:
            print(f"\né…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            print("å°†ä»…æ˜¾ç¤ºæ¨èé…ç½®ï¼Œä¸è¿›è¡Œå¯¹æ¯”")

        optimizer = LlamaCppConfigOptimizer(hardware, config_path)
        recommendation = optimizer.recommend_config()

        if optimizer.current_config:
            optimizer.compare_configs(recommendation.config)

        print("\n" + "=" * 80)
        print("é…ç½®æ¨èå®Œæˆ")
        print("=" * 80)

        print("\næç¤º:")
        print("-" * 80)
        print("1. è¯·å°† 'model.gguf' æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„")
        print("2. æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´ä¸Šä¸‹æ–‡çª—å£å¤§å° (-c)")
        print("3. å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³ï¼Œå¯ä»¥å‡å°æ‰¹å¤„ç†å¤§å° (-b) æˆ–ä¸Šä¸‹æ–‡çª—å£ (-c)")
        print("4. å¯¹äºé•¿æ–‡æœ¬ç”Ÿæˆï¼Œå»ºè®®å¢åŠ ä¸Šä¸‹æ–‡çª—å£å¤§å°")
        print("5. å¯¹äºå®æ—¶å¯¹è¯ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°ä»¥é™ä½å»¶è¿Ÿ")
        print("6. ä½¿ç”¨ --config å‚æ•°æŒ‡å®šä¸åŒçš„é…ç½®æ–‡ä»¶")

    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    import sys

    sys.exit(main())
