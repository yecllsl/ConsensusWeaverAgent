# **智能问答协调终端应用：架构设计文档**

## **1. 架构概述**

本架构设计文档基于需求文档，详细说明智能问答协调终端应用的系统架构、模块划分、组件交互关系及关键技术实现路径。架构设计遵循以下原则：

1. **本地优先**：核心功能采用本地处理，保障隐私和响应速度
2. **模块化设计**：清晰的模块划分，低耦合高内聚
3. **可扩展性**：支持未来功能扩展和性能升级
4. **并发友好**：支持高效的并发任务处理
5. **容错设计**：具备错误处理和恢复机制

## **2. 系统架构层次**

应用采用分层架构设计，从底层到上层依次为：

```
┌─────────────────────────────────────────────────────────────┐
│                   用户界面层 (UI Layer)                      │
│  - 命令行界面 (CLI)                                          │
├─────────────────────────────────────────────────────────────┤
│                   应用服务层 (Service Layer)                 │
│  - 智能交互引擎                                              │
│  - 执行策略管理器                                            │
├─────────────────────────────────────────────────────────────┤
│                   核心功能层 (Core Layer)                     │
│  - 并发查询执行器                                            │
│  - 共识分析引擎                                              │
│  - 报告生成器                                                │
├─────────────────────────────────────────────────────────────┤
│                   基础设施层 (Infrastructure Layer)          │
│  - LLM集成服务                                               │
│  - 外部工具集成                                             │
│  - 数据持久化                                                │
│  - 配置管理                                                  │
│  - 日志系统                                                  │
└─────────────────────────────────────────────────────────────┘
```

## **3. 核心模块设计**

### **3.1 智能交互引擎**

**功能描述**：负责管理与用户的整个对话流程，包括问题接收、澄清、分析和重构。

**组件构成**：

| 组件 | 功能 | 技术实现 |
|------|------|----------|
| **意图接收器** | 接收用户输入的自然语言问题 | Python Click, 标准输入处理 |
| **问题分析器** | 分析问题的完整性、清晰度和歧义 | LangChain, Ollama |
| **澄清对话管理器** | 生成澄清问题并管理对话流程 | LangGraph, 状态管理 |
| **问题重构器** | 重写问题为专业、完整的最终陈述 | LangChain, Ollama |

**交互流程**：
```
用户输入 → 意图接收器 → 问题分析器 → 澄清对话管理器 → 问题重构器 → 用户确认
```

**关键实现细节**：
- 使用LangGraph构建有状态的对话工作流
- 澄清对话最多3轮，支持用户随时终止
- 问题重构后需用户确认才能继续

### **3.2 执行策略管理器**

**功能描述**：根据澄清后的最终问题，判断最优执行路径。

**组件构成**：

| 组件 | 功能 | 技术实现 |
|------|------|----------|
| **复杂度分类器** | 判断问题是简单还是复杂 | LangChain, Ollama |
| **路径决策器** | 选择直接回答或并行查询 | LangGraph, 条件分支 |
| **工具选择器** | 选择合适的外部AI工具 | 配置驱动，策略引擎 |

**决策逻辑**：
```
最终问题 → 复杂度分类器 → 路径决策器 → 
  ├─ 简单问题 → 本地LLM直接回答
  └─ 复杂问题 → 工具选择器 → 并发查询执行器
```

**关键实现细节**：
- 简单问题：常识性、定义性问题，本地知识可回答
- 复杂问题：需要专业知识、实时信息或多源验证
- 工具选择基于问题类型匹配和配置优先级

### **3.3 并发查询执行器**

**功能描述**：负责与外部AI命令行工具的交互，并发执行工具并收集结果。

**组件构成**：

| 组件 | 功能 | 技术实现 |
|------|------|----------|
| **进程管理器** | 并发启动和管理外部工具进程 | Python asyncio, subprocess |
| **输出捕获器** | 捕获工具的标准输出和错误 | subprocess.PIPE |
| **结果标准化器** | 将工具结果统一为标准格式 | 自定义解析器，JSON Schema |
| **错误处理器** | 处理工具执行失败和超时 | 超时监控，错误日志 |

**执行流程**：
```
工具选择 → 进程管理器启动子进程 → 输出捕获 → 结果标准化 → 结果返回
```

**关键实现细节**：
- 最大并发工具数：5个
- 超时时间：60秒
- 单个工具失败不影响其他工具执行
- 结果统一封装为标准化JSON格式

### **3.4 共识分析引擎**

**功能描述**：对收集到的多个答案进行深度分析，计算共识度和提取核心观点。

**组件构成**：

| 组件 | 功能 | 技术实现 |
|------|------|----------|
| **相似度计算器** | 计算答案间的文本相似程度 | scikit-learn, 余弦相似度 |
| **共识度评估器** | 计算每个答案的共识得分 | 基于相似度矩阵的加权平均 |
| **观点提取器** | 提取答案中的核心观点 | NLTK, 关键词提取算法 |
| **分歧识别器** | 识别答案间的差异点 | 文本比较算法 |

**分析流程**：
```
工具结果 → 相似度计算器 → 共识度评估器 → 观点提取器 → 分歧识别器 → 分析结果
```

**关键实现细节**：
- 相似度矩阵：N×N矩阵，N为答案数量
- 共识度评分：0-100分，越高表示共识越强
- 核心观点：包含内容和来源标注
- 分歧点：包含内容和来源标注

### **3.5 报告生成器**

**功能描述**：整合分析结果，生成结构清晰的最终报告。

**组件构成**：

| 组件 | 功能 | 技术实现 |
|------|------|----------|
| **报告结构化器** | 组织报告的结构和内容 | 模板引擎 |
| **摘要生成器** | 生成综合性总结 | LangChain, Ollama |
| **格式渲染器** | 渲染为不同格式的报告 | 文本模板，Markdown支持 |

**报告结构**：
1. 问题描述（原始问题和重构问题）
2. 答案列表（所有工具的原始答案）
3. 共识分析（相似度矩阵、共识度评分）
4. 核心观点（提取的共同点）
5. 分歧点（答案间的差异）
6. 综合结论（最终建议）

**关键实现细节**：
- 支持纯文本格式输出
- 报告生成时间≤20秒
- 清晰的层次结构和排版

## **4. 数据流与网络边界**

### **4.1 核心数据流**

下图说明了应用内的核心数据流和网络边界：

```
┌─────────────────────────────────────────────────────────────┐
│                   用户本地环境                                │
├─────────────────┬───────────────────────────────────────────┤
│  纯本地处理      │        涉及外部网络的操作                   │
│ (无网络请求)     │ (通过子进程调用，由外部CLI发起)              │
├─────────────────┼───────────────────────────────────────────┤
│ 1. 终端用户输入  │                                           │
│ 2. 智能交互引擎  │                                           │
│ 3. 执行策略决策  │                                           │
│ 4. 本地LLM调用   │                                           │
│ 5. 共识分析引擎  │                                           │
│ 6. 报告生成器    │                                           │
│ 7. 终端输出报告  │                                           │
│ 8. SQLite存储   │                                           │
│                 │ 9. 并发查询执行器创建子进程                 │
│                 │ 10. 子进程执行: `iflow ask ...`            │
│                 │ 11. iflow CLI 连接云端API → 返回答案        │
│                 │ 12. 子进程执行: `codebuddy ask ...`        │
│                 │ 13. codebuddy CLI 连接云端API → 返回答案     │
│                 │ ... (其他工具)                            │
├─────────────────┼───────────────────────────────────────────┤
│ 14. 收集子进程输出│                                           │
│ 15. 结果标准化    │                                           │
│ 16. 进入共识分析  │                                           │
└─────────────────┴───────────────────────────────────────────┘
```

**关键说明**：
- 左侧区域为纯本地处理，不依赖互联网
- 右侧区域为涉及外部网络的操作，由外部工具自主发起
- 应用核心进程不直接发起网络请求，仅管理子进程生命周期

### **4.2 网络需求**

| 场景 | 网络需求 | 实现方式 |
|------|----------|----------|
| 用户与本地LLM对话 | 无网络 | 直接调用Ollama本地服务 |
| 工作流决策 | 无网络 | 本地LLM分析判断 |
| 共识分析 | 无网络 | 本地文本处理算法 |
| 调用外部工具 | 需要互联网 | 子进程调用外部CLI工具 |

## **5. 配置管理**

### **5.1 配置文件结构**

应用使用YAML格式的配置文件（config.yaml），包含以下主要部分：

```yaml
# 本地LLM配置
local_llm:
  provider: "ollama"          # LLM提供商
  model: "qwen3:8b"          # 使用的模型名称
  base_url: "http://localhost:11434"  # Ollama服务地址
  timeout: 30                 # LLM调用超时时间

# 外部AI工具配置
external_tools:
  - name: "iflow"            # 工具名称
    command: "iflow"         # 命令名称
    args: "ask --streaming=false"  # 命令参数
    needs_internet: true     # 是否需要互联网
    priority: 1              # 优先级（数字越小优先级越高）
    enabled: true            # 是否启用
  - name: "codebuddy"
    command: "codebuddy"
    args: "ask --format plain"
    needs_internet: true
    priority: 2
    enabled: true

# 网络配置
network:
  check_before_run: true     # 启动前检查网络
  timeout: 60                # 外部工具调用超时时间（秒）

# 应用配置
app:
  max_clarification_rounds: 3  # 最大澄清轮数
  max_parallel_tools: 5        # 最大并发工具数
  log_level: "info"            # 日志级别
  log_file: "consensusweaver.log"  # 日志文件路径
  history_enabled: true        # 是否启用历史记录
  history_limit: 100          # 历史记录限制数量
```

### **5.2 配置加载与管理**

- 使用PyYAML库加载和解析配置文件
- 支持环境变量覆盖配置项
- 配置变更后无需重启应用（热更新）
- 提供命令行参数用于临时覆盖配置

## **6. 开发、部署与扩展**

### **6.1 部署前提**

#### **6.1.1 本地环境**
- Python 3.12+
- Ollama 0.1.0+（已安装并运行所需模型，如qwen3:8b）
- 依赖库：通过uv安装（langchain, langgraph, sqlite3, pyyaml, nltk, scikit-learn, click等）

#### **6.1.2 外部工具环境**
- 所需外部AI CLI工具（如iflow, codebuddy）已安装并配置
- 工具已正确配置网络认证和API密钥
- 工具可通过系统PATH访问
- 运行机器能够访问工具依赖的互联网API端点

### **6.2 部署步骤**

1. 克隆项目代码：`git clone <repository-url>`
2. 安装依赖：`uv install`
3. 配置应用参数：编辑config.yaml
4. 启动本地Ollama服务：`ollama serve`
5. 下载所需模型：`ollama pull qwen3:8b`
6. 运行应用：`python -m src.main`

### **6.3 扩展能力**

#### **6.3.1 新增外部AI工具**
1. 在config.yaml中添加工具配置
2. 确保工具符合标准接口规范
3. 无需修改核心代码

#### **6.3.2 更换本地LLM模型**
1. 在config.yaml中修改local_llm.model配置
2. 使用ollama pull下载新模型
3. 无需修改核心代码

#### **6.3.3 未来GUI扩展**
- 基于PySide6实现图形用户界面
- 复用现有核心模块（core/, chains/, analyzers/, data/）
- 与CLI共享同一套应用服务层
- 保持相同的网络行为模式（本地功能离线、外部工具联网）

## **7. 关键技术实现路径**

### **7.1 本地LLM集成**
1. 使用LangChain的OllamaWrapper连接本地Ollama服务
2. 配置模型参数和超时设置
3. 实现LLM调用的错误处理和重试机制

### **7.2 异步并发处理**
1. 使用Python asyncio实现并发任务调度
2. 使用asyncio.subprocess管理外部工具进程
3. 实现任务超时控制和结果收集

### **7.3 文本相似度计算**
1. 使用scikit-learn的TF-IDF向量器将文本转换为向量
2. 使用余弦相似度计算向量间的相似程度
3. 构建相似度矩阵并可视化（可选）

### **7.4 核心观点提取**
1. 使用NLTK进行文本分词和词性标注
2. 应用关键词提取算法（如TextRank）
3. 对关键词进行聚类和分组
4. 生成结构化的核心观点

### **7.5 错误处理与容错**
1. 实现分层错误处理机制
2. 为外部工具调用添加超时和重试
3. 提供友好的错误提示
4. 记录详细的错误日志

## **8. 性能与安全考量**

### **8.1 性能优化**
- 本地LLM调用优化：模型选择、参数调优
- 并发处理优化：异步IO、进程池管理
- 文本处理优化：算法选择、数据结构优化
- 缓存机制：结果缓存、模型缓存

### **8.2 安全保障**
- 本地数据处理：保护用户隐私
- 工具沙箱：限制外部工具的系统访问权限
- 网络安全：仅允许外部工具发起网络请求
- 错误处理：不暴露敏感信息
- 日志安全：安全的日志记录和存储

## **9. 总结**

本架构设计文档详细说明了智能问答协调终端应用的系统架构、模块划分、组件交互关系及关键技术实现路径。架构设计遵循本地优先、模块化、可扩展、并发友好和容错设计原则，确保能够支撑需求文档中描述的所有功能实现。同时，设计考虑了性能优化和安全保障，为应用的稳定运行和未来扩展奠定了坚实基础。