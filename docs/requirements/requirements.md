# **智能问答协调终端应用：需求文档 (修订版)**

## **1. 项目概述**

### **1.1 项目简介**
本项目旨在开发一款先进的本地终端智能问答协调应用。该应用以用户提出的问题为起点，通过与运行在本地的大语言模型互动，主动引导用户澄清和完善问题的核心意图。随后，应用将智能决策执行路径，可并行调用多个需要联网的外部AI命令行工具获取多角度答案，并对这些答案在本地进行深度分析和共识度计算，最终为用户呈现一份结构清晰、洞察深刻的综合性报告。

本应用的核心逻辑与数据处理均在本地完成（包括LLM对话、工作流编排、文本分析），无需为应用本身部署网络服务接口，充分保障核心交互的隐私与响应速度。所调用的外部AI CLI工具（如iflow, codebuddy，qwen等）需要能够访问互联网以调用其各自的云端模型服务。其核心架构为未来向基于PySide6的图形用户界面扩展奠定了坚实基础。

### **1.2 核心演进阶段**
* **基础功能阶段**：实现并发调用多个需要联网的AI命令行工具并收集结果。
* **智能增强阶段**：集成本地大语言模型，实现纯本地的问题澄清与智能决策。
* **终端整合阶段**：融合前两阶段功能，形成统一、健壮的命令行应用。

### **1.3 核心价值**
1. **提升信息获取效率**：自动化并行查询流程，告别手动操作。
2. **保障答案质量**：通过本地LLM进行交互式问题澄清，确保提问精准性。
3. **提供决策支持**：在本地对来自不同云端工具的结果进行多维度对比和共识分析。
4. **确保核心隐私**：用户的原始问题、澄清对话、以及最终的综合分析过程均在本地处理，仅向用户明确授权的外部工具发送最终确认后的问题。

### **1.4 术语定义**
| 术语 | 定义 |
|------|------|
| 本地LLM | 运行在用户本地环境的大语言模型，通过llama-cpp-python直接加载GGUF模型文件 |
| 外部AI工具 | 需要联网的第三方AI命令行工具（如iflow, codebuddy, qwen等） |
| 简单问题 | 可以通过本地LLM直接回答，不需要外部工具支持的问题 |
| 复杂问题 | 需要调用外部AI工具获取更全面答案的问题 |
| 共识度 | 衡量不同答案之间一致性的量化指标 |

## **2. 功能需求**

### **2.1 智能交互引擎**
该引擎是应用的大脑，完全在本地运行，负责管理与用户的整个对话流程。

#### **2.1.1 问题澄清对话循环**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **意图接收** | 在本地终端接收用户输入的自然语言初始问题 | 1. 成功接收并解析用户的文本输入<br>2. 支持多行输入 |
| **智能分析** | 利用本地LLM分析问题的完整性、清晰度和潜在歧义 | 1. 正确识别问题的完整性（是否包含必要信息）<br>2. 准确判断问题的潜在歧义 |
| **交互式追问** | 基于分析结果，由本地LLM自动生成澄清问题，与用户进行纯本地交互 | 1. 生成的澄清问题针对性强<br>2. 追问次数不超过3次<br>3. 用户可随时终止澄清过程 |
| **问题重构与确认** | 综合用户反馈，由本地LLM重写成一个专业、完整的最终问题陈述，并在本地终端交予用户最终确认 | 1. 重构后的问题准确反映用户意图<br>2. 提供清晰的确认界面<br>3. 用户可编辑重构后的问题 |

**澄清对话终止条件**：
1. 追问次数达到3次上限
2. 用户明确确认问题无需进一步澄清
3. 用户明确拒绝继续澄清
4. 本地LLM判断问题已足够清晰

#### **2.1.2 执行策略决策**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **路径判断** | 根据澄清后的最终问题，由主Agent判断最优执行路径 | 1. 路径判断准确率≥80%<br>2. 决策时间≤15秒 |
| **直接回答** | 对于简单问题，直接由主Agent生成答案 | 1. 简单问题识别准确率≥85%<br>2. 答案生成时间≤30秒 |
| **并行查询** | 对于复杂问题，决策调用一个或多个需要联网的外部AI工具进行并发查询 | 1. 复杂问题识别准确率≥85%<br>2. 正确选择适合的外部工具<br>3. 支持同时调用1-5个外部工具 |

### **2.1.3 命令行参数控制**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **主Agent选择** | 通过命令行参数指定使用不同的外部工具作为主Agent，替代本地LLM的功能 | 1. 支持--i参数：使用iflow作为主Agent<br>2. 支持--q参数：使用qwen作为主Agent<br>3. 支持--b参数：使用codebuddy作为主Agent |
| **参数解析** | 正确解析命令行参数，根据参数选择相应的主Agent | 1. 参数解析准确率100%<br>2. 无参数时默认使用本地LLM作为主Agent |
| **功能替代** | 所选主Agent能够替代本地LLM完成问题澄清、严谨性判断等工作 | 1. 问题澄清功能正常工作<br>2. 执行策略决策准确<br>3. 与本地LLM功能等效 |

**问题复杂度判断标准**：
- **简单问题**：常识性问题、定义性问题、本地知识可回答的问题
- **复杂问题**：需要专业领域知识、实时信息、多源验证的问题

**工具选择策略**：
1. 根据问题类型匹配最适合的工具
2. 优先选择历史表现较好的工具
3. 最多同时调用5个工具

### **2.2 并发查询执行器**
该模块负责与需要联网的外部AI命令行工具的交互。

#### **2.2.1 工具并行化执行**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **并发进程管理** | 能够并发启动并管理多个外部CLI进程 | 1. 支持同时运行1-5个外部工具进程<br>2. 正确管理进程生命周期 |
| **网络隔离** | 每个外部CLI工具进程在运行时，会自主连接其对应的互联网API服务以获取答案。应用本身不处理该网络连接，仅管理子进程的生命周期 | 1. 应用核心进程不直接发起网络请求<br>2. 子进程网络行为由外部工具自身控制 |

**超时策略**：
- 外部工具调用超时时间为60秒
- 超时后自动终止进程并记录错误

**失败处理机制**：
- 单个工具执行失败不影响其他工具
- 记录失败原因并在最终报告中说明
- 不提供自动重试机制

### **2.2.2 结果收集与标准化**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **输出捕获** | 在本地捕获每个子进程的标准输出和标准错误流 | 1. 完整捕获工具的所有输出<br>2. 区分标准输出和错误输出 |
| **结果标准化** | 将结果统一封装为结构化数据 | 1. 所有工具结果采用统一数据结构<br>2. 正确处理不同格式的工具输出 |

### **2.3 共识分析引擎**
该模块在本地对收集到的多个答案进行深度处理。

#### **2.3.1 多维度文本分析**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **相似度矩阵计算** | 在本地量化答案间的文本相似程度 | 1. 使用余弦相似度算法计算文本相似性<br>2. 生成完整的相似度矩阵<br>3. 计算时间≤10秒/5个答案 |
| **共识度评分** | 在本地计算每个答案的"共识得分" | 1. 共识度计算算法：基于相似度矩阵的加权平均<br>2. 评分范围0-100<br>3. 计算时间≤5秒 |
| **核心观点提取** | 在本地识别并提取多个答案中的共同点 | 1. 正确提取核心观点（准确率≥80%）<br>2. 每个核心观点包含来源标注<br>3. 处理时间≤10秒 |

#### **2.3.2 智能综合与报告生成**

| 功能点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **答案融合** | 在本地生成一份融合了各方要点的综合性总结摘要 | 1. 融合结果涵盖所有核心观点<br>2. 逻辑清晰，语言流畅<br>3. 生成时间≤15秒 |
| **分歧点标注** | 在本地明确指出来源答案之间的差异 | 1. 准确识别分歧点（准确率≥75%）<br>2. 提供分歧点的详细说明<br>3. 标注分歧点的来源 |
| **生成最终报告** | 在本地整合所有分析结果，形成最终报告 | 1. 报告结构完整，包含：问题描述、答案列表、共识分析、综合结论、分歧点<br>2. 支持纯文本格式输出<br>3. 生成时间≤20秒 |

## **3. 非功能需求**

### **3.1 性能需求**
| 需求点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **响应时间** | 系统各环节的响应时间需满足用户体验要求 | 1. 本地LLM响应时间≤5秒<br>2. 外部工具调用超时时间≤60秒<br>3. 共识分析处理时间≤20秒<br>4. 最终报告生成时间≤20秒 |
| **吞吐量** | 系统能够同时处理的请求数量 | 1. 支持同时处理1个用户会话<br>2. 支持并发调用1-5个外部工具 |

### **3.2 安全性需求**
| 需求点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **数据隐私** | 用户数据的处理需确保隐私安全 | 1. 原始问题、澄清对话、分析过程均在本地处理<br>2. 仅向外部工具发送最终确认后的问题<br>3. 不存储用户的原始问题和答案 |
| **工具验证** | 确保外部工具的安全性和可靠性 | 1. 外部工具需在配置文件中明确声明<br>2. 提供工具权限控制机制<br>3. 限制工具的系统访问权限 |
| **错误处理** | 安全处理系统错误和异常 | 1. 不向用户暴露敏感的错误信息<br>2. 所有错误均记录在本地日志中<br>3. 提供友好的错误提示 |

### **3.3 可用性需求**
| 需求点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **易用性** | 用户界面简洁直观，易于操作 | 1. 命令行界面清晰易用<br>2. 提供详细的帮助信息<br>3. 支持快捷键操作 |
| **容错性** | 系统能够处理各种异常情况 | 1. 外部工具执行失败不影响系统运行<br>2. 网络连接失败时提供友好提示<br>3. 本地LLM服务不可用时提供降级方案 |
| **兼容性** | 支持多种操作系统和环境 | 1. 支持Windows 10/11<br>2. 支持Python 3.12+<br>3. 支持llama-cpp-python 0.2.0+ |

### **3.4 可扩展性需求**
| 需求点 | 详细描述 | 验收标准 |
|--------|----------|----------|
| **工具扩展** | 能够方便地添加新的外部AI工具 | 1. 通过配置文件即可添加新工具<br>2. 无需修改核心代码<br>3. 支持自定义工具参数 |
| **模型扩展** | 支持不同的本地LLM模型 | 1. 通过配置文件切换模型<br>2. 支持主流的开源LLM模型<br>3. 无需修改核心代码 |

## **4. 数据需求**

### **4.1 数据结构**

#### **4.1.1 标准化工具结果**
```json
{
  "tool_name": "string",         // 工具名称
  "success": "boolean",         // 执行是否成功
  "answer": "string",           // 工具返回的答案
  "error_message": "string",     // 错误信息（如果执行失败）
  "execution_time": "number",    // 执行时间（毫秒）
  "timestamp": "string"          // 执行时间戳
}
```

#### **4.1.2 共识分析结果**
```json
{
  "similarity_matrix": "array",  // 答案相似度矩阵
  "consensus_scores": "object",  // 每个答案的共识度得分
  "key_points": [                 // 提取的核心观点
    {
      "content": "string",       // 观点内容
      "sources": ["string"]      // 观点来源（工具名称列表）
    }
  ],
  "differences": [                // 识别的分歧点
    {
      "content": "string",       // 分歧内容
      "sources": ["string"]      // 分歧来源（工具名称列表）
    }
  ]
}
```

#### **4.1.3 最终报告**
```json
{
  "original_question": "string", // 用户原始问题
  "refined_question": "string",  // 重构后的问题
  "timestamp": "string",         // 报告生成时间戳
  "tool_results": [               // 各工具结果
    "标准化工具结果对象"
  ],
  "consensus_analysis": "共识分析结果对象",  // 共识分析结果
  "comprehensive_summary": "string",  // 综合总结
  "final_conclusion": "string"         // 最终结论
}
```

### **4.2 配置数据**
应用配置文件（config.yaml）应包含以下内容：
```yaml
local_llm:
  provider: "ollama"
  model: "qwen3:8b"
  base_url: "http://localhost:11434"

external_tools:
  - name: "iflow"
    command: "iflow"
    args: "ask --streaming=false"
    needs_internet: true
    priority: 1
  - name: "codebuddy"
    command: "codebuddy"
    args: "ask --format plain"
    needs_internet: true
    priority: 2

network:
  check_before_run: true
  timeout: 30

app:
  max_clarification_rounds: 3
  max_parallel_tools: 5
  log_level: "info"
```

## **5. 验收标准**

### **5.1 功能验收**
1. **智能交互引擎**：
   - 成功接收用户问题并进行智能分析
   - 准确生成澄清问题，追问次数不超过3次
   - 正确重构问题并获得用户确认
   - 准确判断问题复杂度并选择执行路径

2. **并发查询执行器**：
   - 成功并发调用1-5个外部工具
   - 正确管理工具进程生命周期
   - 完整捕获工具输出并标准化

3. **共识分析引擎**：
   - 正确计算答案相似度矩阵
   - 准确生成共识度评分
   - 有效提取核心观点和分歧点
   - 生成结构完整的最终报告

### **5.2 非功能验收**
1. **性能验收**：
   - 本地LLM响应时间≤5秒
   - 外部工具调用超时时间≤60秒
   - 共识分析处理时间≤20秒
   - 最终报告生成时间≤20秒

2. **安全性验收**：
   - 原始问题和澄清对话仅在本地处理
   - 外部工具仅接收最终确认后的问题
   - 所有错误信息不包含敏感内容

3. **可用性验收**：
   - 命令行界面清晰易用
   - 提供详细的帮助信息
   - 外部工具执行失败不影响系统运行

## **6. 部署与维护要求**

### **6.1 部署前提**
1. **本地环境**：
   - Python 3.12+
   - llama-cpp-python 0.2.0+（已安装）
   - GGUF格式的本地模型文件（如Qwen3-8B-Q5_K_M.gguf）
   - 依赖库：langchain, langgraph, sqlite3等

2. **外部工具环境**：
   - 所需外部AI CLI工具（如iflow, codebuddy，qwen）已安装并配置
   - 工具已正确配置网络认证和API密钥
   - 工具可通过系统PATH访问

### **6.2 部署步骤**
1. 克隆项目代码
2. 使用uv安装依赖：`uv install`
3. 配置应用参数：编辑config.yaml
4. 确保GGUF模型文件已下载并在config.yaml中正确配置路径（如果使用本地LLM作为主Agent）
5. 运行应用：
   - 使用默认本地LLM作为主Agent：`python -m src.main`
   - 使用iflow作为主Agent：`python -m src.main --i`
   - 使用qwen作为主Agent：`python -m src.main --q`
   - 使用codebuddy作为主Agent：`python -m src.main --b`

### **6.3 维护要求**
1. **日志管理**：
   - 系统日志存储在本地文件中
   - 日志包含错误信息、执行状态等
   - 定期清理旧日志（保留最近30天）

2. **更新与升级**：
   - 定期更新依赖库：`uv update`
   - 支持热更新配置文件
   - 提供版本升级指南

3. **故障排查**：
   - 提供详细的故障排查文档
   - 常见问题及解决方案
   - 错误代码说明

### **6.4 支持与帮助**
1. 提供详细的用户手册
2. 命令行帮助：`python -m src.main --help`
3. 常见问题解答（FAQ）